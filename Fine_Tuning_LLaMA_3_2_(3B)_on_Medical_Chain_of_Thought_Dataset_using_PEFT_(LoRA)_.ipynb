{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXuh3imf6jib"
      },
      "outputs": [],
      "source": [
        "# 2. Imports and Environment Setup\n",
        "import os\n",
        "import torch\n",
        "import wandb\n",
        "from huggingface_hub import login, create_repo, upload_folder\n",
        "\n",
        "# GPU Check\n",
        "if not torch.cuda.is_available():\n",
        "    raise SystemError(\"GPU not available! Enable GPU in Kaggle settings.\")\n",
        "print(\"GPU is available.\")\n",
        "\n",
        "# Weights & Biases Login\n",
        "wandb_key = os.environ.get('your key')\n",
        "if wandb_key:\n",
        "    wandb.login(key=wandb_key)\n",
        "else:\n",
        "    print(\"WANDB_API_KEY not set. Set it as Kaggle secret.\")\n",
        "\n",
        "# Hugging Face Login\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "hf_token = os.getenv(\"HF_TOKEN\")\n",
        "if hf_token:\n",
        "    login(token=hf_token)\n",
        "else:\n",
        "    print(\"HF_TOKEN not set. Set it as Kaggle secret.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6ZOLtHE3PAz"
      },
      "outputs": [],
      "source": [
        "# 3. Load & Format Dataset\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"FreedomIntelligence/Medical-CoT\")\n",
        "\n",
        "def format_example(ex):\n",
        "    think = ex.get(\"think\", \"\")\n",
        "    response = ex.get(\"response\", \"\")\n",
        "    return f\"<think>{think}</think> <response>{response}</response>\"\n",
        "\n",
        "formatted_ds = dataset.map(lambda x: {\"text\": format_example(x)})\n",
        "\n",
        "val_data = formatted_ds[\"train\"].select(range(100))\n",
        "train_data = formatted_ds[\"train\"].select(range(100, len(formatted_ds[\"train\"])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArlCwr0A6gVz"
      },
      "outputs": [],
      "source": [
        "# 4. Load LLaMA 3.2 3B Model with LoRA from Unsloth\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/llama-3-3b-bnb-4bit\",\n",
        "    max_seq_length=4096,\n",
        "    dtype=torch.float16,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4mPczl43WTa"
      },
      "outputs": [],
      "source": [
        "# 5. Tokenize and Preprocess Data\n",
        "from datasets import Dataset\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "train_ds = Dataset.from_list(train_data)\n",
        "val_ds = Dataset.from_list(val_data)\n",
        "\n",
        "def tokenize_fn(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "train_ds = train_ds.map(tokenize_fn, batched=True)\n",
        "val_ds = val_ds.map(tokenize_fn, batched=True)\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cBmYifp3aTr"
      },
      "outputs": [],
      "source": [
        "# 6. Training the Model with wandb Logging\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from evaluate import load\n",
        "\n",
        "rouge = load(\"rouge\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    gradient_accumulation_steps=4,\n",
        "    logging_dir=\"./logs\",\n",
        "    report_to=\"wandb\",\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    return rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bc6H99fq3dnr"
      },
      "outputs": [],
      "source": [
        "# 7. Save Model Adapter and Tokenizer\n",
        "model.save_pretrained(\"lora_medical_model\")\n",
        "tokenizer.save_pretrained(\"lora_medical_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vDDvJAz3fPC"
      },
      "outputs": [],
      "source": [
        "# 8. Upload to Hugging Face Hub\n",
        "repo_id = \"ArshiaJ05/lora-medical-llama3-3b\"\n",
        "create_repo(repo_id, exist_ok=True)\n",
        "\n",
        "upload_folder(\n",
        "    folder_path=\"lora_medical_model\",\n",
        "    repo_id=repo_id,\n",
        "    repo_type=\"model\",\n",
        ")\n",
        "\n",
        "print(f\"Model and tokenizer uploaded to https://huggingface.co/{repo_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pxfzt6fv3grM"
      },
      "outputs": [],
      "source": [
        "# 9. Load Model & Adapter for Inference\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"unsloth/llama-3-3b-bnb-4bit\")\n",
        "peft_model = PeftModel.from_pretrained(base_model, repo_id)\n",
        "tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
        "\n",
        "input_text = \"<think>Patient reports nausea and vomiting...</think>\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\").to(peft_model.device)\n",
        "\n",
        "outputs = peft_model.generate(**inputs, max_new_tokens=100)\n",
        "print(\"Generated response:\")\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sx5LnY9s3jJU"
      },
      "outputs": [],
      "source": [
        "# 10. ROUGE-L Score Comparison\n",
        "# Placeholder for before/after comparison\n",
        "preds_before = [\"<response>Sample prediction before</response>\"]\n",
        "preds_after = [\"<response>Sample prediction after</response>\"]\n",
        "refs = [\"<response>Ground truth reference</response>\"]\n",
        "\n",
        "score_before = rouge.compute(predictions=preds_before, references=refs, use_stemmer=True)\n",
        "score_after = rouge.compute(predictions=preds_after, references=refs, use_stemmer=True)\n",
        "\n",
        "print(f\"ROUGE-L before fine-tuning: {score_before['rougeL']}\")\n",
        "print(f\"ROUGE-L after fine-tuning: {score_after['rougeL']}\")\n",
        "\n",
        "# End of Notebook\n",
        "print(\"Notebook complete. Deliverables include Kaggle notebook, wandb logs, HF repo, and evaluation results.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# You can run this code in colab"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
